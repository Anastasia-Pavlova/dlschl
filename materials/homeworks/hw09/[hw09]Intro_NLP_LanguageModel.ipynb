{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Школа глубокого обучения\n",
    "\n",
    "<a href=\"https://mipt.ru/science/labs/laboratoriya-neyronnykh-sistem-i-glubokogo-obucheniya/\"><img align=\"right\" src=\"https://avatars1.githubusercontent.com/u/29918795?v=4&s=200\" alt=\"DeepHackLab\" style=\"position:relative;top:-40px;right:10px;height:100px;\" /></a>\n",
    "\n",
    "\n",
    "\n",
    "### Физтех-Школа Прикладной математики и информатики МФТИ \n",
    "### Лаборатория нейронных сетей и глубокого обучения (DeepHackLab)\n",
    "*Дедлайн -- 30 апреля.*§\n",
    "\n",
    "Хахулин Тарас (ФРТК МФТИ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 9.\n",
    "\n",
    "## Погружение в Natural Language Proccessing\n",
    "\n",
    "Так как обработкой языка занимались давно, а нейронные сети пытаются вставить везде лишь лет 10, то прежде чем решать задачи с помощью NN хотелось бы попробовать решить ее классическими методами. \n",
    "\n",
    "\n",
    "В данном задании мы будем всеми силами пытаться построить языковую модель (language model). \n",
    "\n",
    "Что же такое языковая модель? Сейчас прольем немного математики, а ниже опишем обычными словами все это.\n",
    "\n",
    "Допустим, у нас есть огромный текстовый корпус содержащий какой-либо смысл и нам хотелось бы уметь создавать такой текст с помощью обученной модели. Для этого нам необходимо знать вероятность $p(W) = p(w_1, w_2, w_3,..) $ появления предложения или последовательности слов или же нам интересно получить вероятность $p(w_5| w_1, w_2, w_3, w_4)$ появления слова при контексте. Будем называть модели, которые считают такие вероятности языковыми моделями.\n",
    "\n",
    "#### Что это за модель? \n",
    "\n",
    "Тут везде почему-то говорится про какую-то неведомую модель. Уточню, вообще говоря, модель (в статистическом смысле, конечно) является математическим представлением процесса. Почти всегда модели являются приближением процесса. Для этого есть несколько причин, но два наиболее важных:\n",
    "1. Обычно мы наблюдаем только процесс ограниченное количество раз\n",
    "2. Модель может быть исключительно сложной, поэтому мы ее упрощаем\n",
    "\n",
    "Вот что обычно делает модель: она описывает, как моделируемый процесс __создает данные__. В нашем случае моделируемым явлением является человеческий язык. Языковая модель дает нам способ генерации человеческого языка. Эти модели обычно составлены из вероятностных распределений.\n",
    "\n",
    "Модель построена путем наблюдения некоторых образцов, сгенерированных явлением, которое должно быть смоделировано. Таким же образом, языковая модель строится путем наблюдения за некоторым текстом.\n",
    "\n",
    "\n",
    "Фуух, надеюсь, теперь стало немного понятно и можно переходить к моделированию.(Не волнуйтесь нейронных моделек будет все равно больше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words (мешок слов) \n",
    "\n",
    "Так как мы решили немного проникнуться nlp, стоит обязательно познать абстракцию мешка слов.  Вообще эта идея, есть наиболее простой способ сопоставления слова-вектор( ведь компьютер у нас может работать только с цифрами, поэтому надо каким-то образом уметь превращать буквы/слова в них) Это, безусловно, самый простой способ моделирования человеческого языка. Это не значит, что это бесполезно и непопулярно.\n",
    "\n",
    "Идея очень проста: посчитаем частоту встречания слов в словаре и составим из этого один вектор.\n",
    "\n",
    "![img](https://hsto.org/getpro/habr/post_images/b91/46a/5be/b9146a5be315f422479e40d85e995289.jpg)\n",
    "\n",
    "Попробуем закодить для настоящего корпуса собраний сочинений Федора Михайловича Достоевского"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "with io.open('dostoevsky.txt', 'r',encoding='utf8') as f:\n",
    "    text = f.read().replace(u'\\xa0', u' ').replace(u'\\ufeff','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sents_full = sent_tokenize(text)\n",
    "sents = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from collections import Counter\n",
    " \n",
    "counts = Counter(words)\n",
    "total_count = len(words)\n",
    " \n",
    "# Print the most common 20 words hint: use counter methods\n",
    " \n",
    "# Compute the frequencies hint: you know total count\n",
    "\n",
    "# The frequencies should sum up to 1 (approximately)\n",
    "print(sum(counts.values()))  # 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    " \n",
    "# Generate 100 words of language\n",
    "new_text = []\n",
    " \n",
    "for _ in range(100):\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word, freq in counts.items():\n",
    "        accumulator += freq\n",
    " \n",
    "        if accumulator >= r:\n",
    "            new_text.append(word)\n",
    "            break\n",
    "\n",
    "print(' '.join(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы можете видеть, это не самый выразительный фрагмент контента. Полученный текст следует только частотным правилам языка и не более того.\n",
    "\n",
    "\n",
    "#### Биграммы,  триграммы и n-граммы\n",
    "\n",
    "Одна идея, которая может помочь нам создать более качественный текст, состоит в том, чтобы убедиться, что новое слово, которое мы добавляем в последовательность, хорошо сочетается с уже существующими словами. Проверка правильности слова после 10 слов может немного переборщить. Мы можем упростить все, чтобы проблема была разумной. Давайте сделаем так, чтобы новое слово получилось после последнего слова в последовательности (модель биграмм) или двух последних слов (модель триграмм) ( можно так долго продолжат, из эмпирических наблюдений при n=5 модель всегда выдает нормальный текст ) .\n",
    "\n",
    "«Биграм» - это причудливое имя для двух последовательных слов, в то время как триграмма (как вы уже догадались) триплет последовательных слов. Вот некоторые быстрые магии NLTK (библиотека для nlp) для извлечения биграмм / триграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    " \n",
    "first_sentence = word_tokenize(sents[0])\n",
    "print(first_sentence)\n",
    "\n",
    "# Get the bigrams\n",
    "print(list(bigrams(first_sentence)))\n",
    "\n",
    "# Get the padded bigrams\n",
    "print(list(bigrams(first_sentence, pad_left=True, pad_right=True)))\n",
    " \n",
    "# Get the trigrams\n",
    "print(list(trigrams(first_sentence)))\n",
    " \n",
    "# Get the padded trigrams\n",
    "print(list(trigrams(first_sentence, pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно и модель над этими биграммами построить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in sents:\n",
    "    sentence = word_tokenize(sentence)\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "        \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[\"что\", \"еще\"]['вам'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[\"мухи\", \"сочинили\"]['слона'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[\"прямо\", \"по\"]['коридорчику'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете поискать еще интересных сочитаний в модели. Например найти самое часто встречаемое в тексте выражение. Или попробовать использовать пятиграммную модель. Это довольно интересно. \n",
    "\n",
    "Ниже, наконец представлен код для нашей генеративной модели. Рекомендую с ним так же поиграться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    for word in model[tuple(new_text[-2:])].keys():\n",
    "        accumulator += model[tuple(new_text[-2:])][word]\n",
    "        if accumulator >= r:\n",
    "            new_text.append(word)\n",
    "            break\n",
    " \n",
    "    if new_text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in new_text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном этапе моедль уже может генерировать более осмысленный текст. Кроме того существует множество статистических методом, которые могают сгладить распределение слов, бороться с появление новых n-грамм. И такие модели до сих пор могут спокойно соревноваться с нейронными решениями. \n",
    "\n",
    "Собственно покатили в нейронный мир."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-RNN in PyTorch\n",
    "\n",
    "Тут нам предстоит построить character-level RNN c помощью PyTroch. Для того чтобы освежить свои знания о данной модели рекомендую к прочтению [сию статью](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) написанную Андреем Карпатым.  Сразу проясню, тут мы будем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import codecs\n",
    "import io\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обновите текст, если что-то потерлось. Можно сделать это не только в памяти компьютера, но и в своей собственной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('dostoevsky.txt', 'r',encoding='utf8') as f:\n",
    "    text = f.read().replace(u'\\xa0', u' ').replace(u'\\ufeff','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируем наш текст в цифры, как мы и обсуждали ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка данных\n",
    "\n",
    "Будем использовать для представления букв one-hot вектора. Напишите функцию для этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать функцию ниже, которая строит генератор мини-батчей. Каждая последовательность будет длины `n_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''\n",
    "    Create a _generator_ that returns mini-batches of size\n",
    "        n_seqs x n_steps from array.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение charRNN модели\n",
    "\n",
    "Ниже вам будет необходимо написать свою char-rnn по данному описанию. Как всегда основные рекомендации: сначала пишем определение слоев в  init, затем описываем их вызов в forward.\n",
    "\n",
    "\n",
    "Кроме того тут есть важная функция predict. (в блоге Карпатого очень подробно все описано и дабы не заниматься копипастой, я направляю вас туда). Для начала попробуйте проверить учится ли ваша сеть и только потом заполняйте метод predict.\n",
    "\n",
    "Почему даем именно модель над символами? Потому что это очень простая структура, которая может быть применена не только к текстам, но и к музыке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.1, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = <dropout>\n",
    "        self.lstm = <torch.RNN>\n",
    "        self.fc = <linear>\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "\n",
    "        return x, h\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=True, top_k=None):\n",
    "        ''' \n",
    "        Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = <one-hot>\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "         \n",
    "        # Create h\n",
    "        # Get h and out from net\n",
    "        \n",
    "        # Predict next character using softmax\n",
    "\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            predict, top_ch = predict.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        predict = predict.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, predict=predict/predict.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        # FC weights as random uniform\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # initialized to zero, for hidden state and cell state of RNN\n",
    "        weight = next(self.parameters()).data\n",
    "        return #code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки функционирования у нас есть функция `train` , которая позволит провести вам большое число экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.0005, clip=3, val_frac=0.1, cuda=False, print_every=200):\n",
    "    ''' \n",
    "    Traing a network \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    net: CharRNN network\n",
    "    data: text data to train the network\n",
    "    epochs: Number of epochs to train\n",
    "    n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "    n_steps: Number of character steps per mini-batch\n",
    "    lr: learning rate\n",
    "    clip: gradient clipping\n",
    "    val_frac: Fraction of data to hold out for validation\n",
    "    cuda: Train with CUDA on a GPU\n",
    "    print_every: Number of steps for printing training and validation loss\n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = Variable(h.data)\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    #val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    val_h = Variable(val_h.data)\n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Время тренировки!\n",
    "\n",
    " \n",
    "Теперь мы можем тренировать сеть. Сначала мы создадим саму сеть, с некоторыми заданными гиперпараметрами. Затем определите размеры мини-партий (количество последовательностей и количество шагов) и начните обучение. С функцией поезда мы можем установить количество эпох, скорость обучения и другие параметры. Кроме того, мы можем запустить обучение на графическом процессоре, установив `cuda = True`. Сейчас google дает всем бесплатно использовать gpu с помощью сервиса codelab.\n",
    "\n",
    "\n",
    "![a](https://www.apmpodcasts.org/wp-content/uploads/2015/06/adventure-time.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=2048, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=15, n_seqs=n_seqs, n_steps=n_steps, lr=0.0005, cuda=True, print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train](https://i.pinimg.com/474x/0e/58/69/0e5869297852211e8447d6b09fa1f4f5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка модели \n",
    "\n",
    "Чтобы настроить гиперпараметры для получения максимальной производительности, вам понадобятся наблюдения за обучением и валидацией. Если ваша потеря обучения намного ниже, чем потеря проверки, вы перерабатываете. Увеличьте регуляризацию (больше выпадений) или используйте меньшую сеть. Если потери обучения и проверки близки, вы недофинансируете, чтобы увеличить размер сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения мы сохраним модель, чтобы мы могли загрузить ее позже, если понадобится. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметров скрытого слоя и текстовых символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('./rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семплирование\n",
    "\n",
    "Теперь, когда модель обучена, мы захотим попробовать ее. Чтобы заполучить текст, мы передаем символ и сеть прогнозируем следующий символ. Затем мы берем новый символ, передаем его обратно и получаем еще один. Просто продолжайте делать это, и вы создадите кучу текста!\n",
    "\n",
    "### Top K \n",
    "\n",
    "Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным признакам. Мы можем сделать выборку более разумным, но менее переменным, учитывая только некоторые вероятные символы $ K $. Это будет препятствовать тому, чтобы сеть давала нам абсолютно абсурдные символы, позволяя ей вводить некоторый шум и случайность в выбранный текст.\n",
    "\n",
    "Как правило, вы хотите настроить сеть, чтобы создать скрытое состояние. В противном случае сеть начнет генерировать символы наугад.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из основного задания, попробуйте сгенерировать небольшой рассказик. Поиграйте с параметрами. Найдите собственный датасет( чтобы проверяющим было весело ) и попробуйте обучить нейронную сеть на нем. \n",
    "\n",
    "Не забывайте эксперементировать со структурой. У LSTM много аналогий, посмотрите как они справляются с данной задачей и запишите это в отчет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(net, 10, prime='Путин', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отчет\n",
    "\n",
    "\n",
    "Тут предлагается описать все проделанные эксперименты. Отчет будет весить 5 баллов из 10 ( работающая сеть еще 5 ).  Попробуйте сделать его насыщенным необходимой информацией для проверяющих, чтобы можно было сразу оценить сколько было проделано работы. Пример вопросов для отчета был дан выше. Удачи!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
